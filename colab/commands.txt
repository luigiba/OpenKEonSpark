#commands to run OpenKEonSpark on Google Colaboratory platform
#before running, create a folder in your Google Drive named DBpedia
#uplod in the folder the zip archive containing the dataset given as output from generate.py

#install
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
!sudo apt-get install scala
!wget -q https://archive.apache.org/dist/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz 
!tar xf spark-2.1.1-bin-hadoop2.7.tgz
!python3 -m pip install tensorflowonspark py4j pyspark

#connect Google Drive
from google.colab import drive
drive.mount('/content/drive')

#clone project
!git clone https://github.com/luigiba/OpenKEonSpark.git
!mv OpenKEonSpark/colab/gpu_info.py /usr/local/lib/python3.6/dist-packages/tensorflowonspark/gpu_info.py

#unzip dataset
!unzip /content/drive/My\ Drive/DBpedia/10.zip -d /content/drive/My\ Drive/DBpedia/

#set env vars
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ['SPARK_WORKER_INSTANCES'] = '3'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'
os.environ["CUDA_VISIBLE_DEVICES"]="0"
os.environ["CORES_PER_WORKER"] = "1"
os.environ["MEMORY_PER_WORKER"] = "4g"
os.environ["LIB_CUDA"] = "/usr/local/cuda-10.0/lib64"
os.environ["WORK_DIR_PREFIX"] = "/content/OpenKEonSpark"
os.environ["SPARK_HOME"] = "/content/spark-2.1.1-bin-hadoop2.7"

#execute train-evaluate pipeline
!bash $WORK_DIR_PREFIX/colab/run_dbpedia.sh 10 64 "TransE" 0.00001





